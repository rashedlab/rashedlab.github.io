
@Article{info16070517,
AUTHOR = {Darwish, Ahmed M. and Rashed, Essam A. and Khoriba, Ghada},
TITLE = {Mitigating LLM Hallucinations Using a Multi-Agent Framework},
JOURNAL = {Information},
VOLUME = {16},
YEAR = {2025},
NUMBER = {7},
ARTICLE-NUMBER = {517},
URL = {https://www.mdpi.com/2078-2489/16/7/517},
ISSN = {2078-2489},
ABSTRACT = {The rapid advancement of Large Language Models (LLMs) has led to substantial investment in enhancing their capabilities and expanding their feature sets. Despite these developments, a critical gap remains between model sophistication and their dependable deployment in real-world applications. A key concern is the inconsistency of LLM-generated outputs in production environments, which hinders scalability and reliability. In response to these challenges, we propose a novel framework that integrates custom-defined, rule-based logic to constrain and guide LLM behavior effectively. This framework enforces deterministic response boundaries while considering the modelâ€™s reasoning capabilities. Furthermore, we introduce a quantitative performance scoring mechanism that achieves an 85.5% improvement in response consistency, facilitating more predictable and accountable model outputs. The proposed system is industry-agnostic and can be generalized to any domain with a well-defined validation schema. This work contributes to the growing research on aligning LLMs with structured, operational constraints to ensure safe, robust, and scalable deployment.},
DOI = {10.3390/info16070517}
}



